--- auth/authOptions.ts ---
// This file contains NextAuth.js configuration options
// It's imported by both server components and API routes

import { NextAuthOptions } from "next-auth";
import GoogleProvider from "next-auth/providers/google";

export const authOptions: NextAuthOptions = {
  providers: [
    GoogleProvider({
      clientId: process.env.GOOGLE_CLIENT_ID as string,
      clientSecret: process.env.GOOGLE_CLIENT_SECRET as string,
      authorization: {
        params: {
          prompt: "consent",
          access_type: "offline",
          response_type: "code",
          scope:
            "openid profile email https://www.googleapis.com/auth/drive.file",
        },
      },
    }),
  ],
  callbacks: {
    async jwt({ token, account }) {
      // Persist the OAuth access_token to the token right after sign in
      if (account) {
        token.accessToken = account.access_token;
        token.refreshToken = account.refresh_token;
        token.idToken = account.id_token;
      }
      return token;
    },
    async session({ session, token }) {
      // Send properties to the client, like an access_token from a provider
      session.accessToken = token.accessToken;
      session.refreshToken = token.refreshToken;
      session.idToken = token.idToken;
      return session;
    },
  },
};

--- auth/wallet.ts ---
import { mokshaTestnet, vanaMainnet } from "@/contracts/chains";
import { createConfig, http } from "wagmi";
import { coinbaseWallet, injected } from "wagmi/connectors";

// Configure Wagmi
const config = createConfig({
  chains: [mokshaTestnet, vanaMainnet],
  connectors: [
    injected(), // MetaMask and browser injected wallets
    coinbaseWallet({
      appName: "Vana DLP Template",
    }),
  ],
  transports: {
    [mokshaTestnet.id]: http(),
    [vanaMainnet.id]: http(),
  },
});

export const wagmiConfig = config;

--- crypto/utils.ts ---
import eccrypto from "eccrypto";
import * as openpgp from "openpgp";

/**
 * Client-side encryption of file data
 * @param data The data to encrypt
 * @param signature The signature to use for encryption
 * @returns The encrypted data as a Blob
 */
export async function clientSideEncrypt(
  data: Blob,
  signature: string
): Promise<Blob> {
  const dataBuffer = await data.arrayBuffer();
  const message = await openpgp.createMessage({
    binary: new Uint8Array(dataBuffer),
  });

  const encrypted = await openpgp.encrypt({
    message,
    passwords: [signature],
    format: "binary",
  });

  // Convert WebStream<Uint8Array> to Blob
  const response = new Response(encrypted as ReadableStream<Uint8Array>);
  const arrayBuffer = await response.arrayBuffer();
  const uint8Array = new Uint8Array(arrayBuffer);

  const encryptedBlob = new Blob([uint8Array], {
    type: "application/octet-stream",
  });
  return encryptedBlob;
}

/**
 * Encrypts data using a wallet public key
 * @param data The data to encrypt
 * @param publicKey The wallet public key
 * @returns The encrypted data as a hex string
 */
export const encryptWithWalletPublicKey = async (
  data: string,
  publicKey: string
): Promise<string> => {
  // Get consistent encryption parameters
  const { iv, ephemeralKey } = getEncryptionParameters();

  const publicKeyBytes = Buffer.from(
    publicKey.startsWith("0x") ? publicKey.slice(2) : publicKey,
    "hex"
  );
  const uncompressedKey =
    publicKeyBytes.length === 64
      ? Buffer.concat([Buffer.from([4]), publicKeyBytes])
      : publicKeyBytes;

  const encryptedBuffer = await eccrypto.encrypt(
    uncompressedKey,
    Buffer.from(data),
    {
      iv: Buffer.from(iv),
      ephemPrivateKey: Buffer.from(ephemeralKey),
    }
  );

  const encryptedHex = Buffer.concat([
    encryptedBuffer.iv,
    encryptedBuffer.ephemPublicKey,
    encryptedBuffer.ciphertext,
    encryptedBuffer.mac,
  ]).toString("hex");

  return encryptedHex;
};

/**
 * Prepares a file ID for the VANA DLP registry
 * @param url The URL of the file
 * @param timestamp Optional timestamp
 * @returns A formatted file ID
 */
export function formatVanaFileId(
  url: string,
  timestamp: number = Date.now()
): string {
  return `vana_submission_${timestamp}_${url.substring(
    url.lastIndexOf("/") + 1
  )}`;
}

// Store the generated values so they remain consistent
let generatedIV: Uint8Array | null = null;
let generatedEphemeralKey: Uint8Array | null = null;

/**
 * Generate or retrieve the encryption parameters (IV and ephemeral key)
 * Ensures the same values are used across multiple calls
 * @returns An object containing the IV and ephemeral key
 */
export function getEncryptionParameters() {
  if (!generatedIV || !generatedEphemeralKey) {
    // 16-byte initialization vector (fixed value)
    generatedIV = new Uint8Array([
      0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a, 0x0b, 0x0c,
      0x0d, 0x0e, 0x0f, 0x10,
    ]);

    // 32-byte ephemeral key (fixed value)
    generatedEphemeralKey = new Uint8Array([
      0x11, 0x22, 0x33, 0x44, 0x55, 0x66, 0x77, 0x88, 0x99, 0xaa, 0xbb, 0xcc,
      0xdd, 0xee, 0xff, 0x00, 0x10, 0x20, 0x30, 0x40, 0x50, 0x60, 0x70, 0x80,
      0x90, 0xa0, 0xb0, 0xc0, 0xd0, 0xe0, 0xf0, 0x00,
    ]);
  }

  return {
    iv: generatedIV,
    ephemeralKey: generatedEphemeralKey,
    ivHex: Buffer.from(generatedIV).toString("hex"),
    ephemeralKeyHex: Buffer.from(generatedEphemeralKey).toString("hex"),
  };
}

--- google/googleApi.ts ---
// NOTE: This file should only be imported in server components or API routes
// as it uses Node.js specific modules like googleapis
import { google } from "googleapis";
import { Readable } from "stream";

export type GoogleUserInfo = {
  id: string;
  email: string;
  name: string;
  picture: string;
  locale: string;
  verifiedEmail: boolean;
};

export type GoogleDriveInfo = {
  totalStorageBytes: string;
  usedStorageBytes: string;
  percentUsed: number;
  trashBytes: string;
};

export async function getUserInfo(
  accessToken: string
): Promise<GoogleUserInfo> {
  const response = await fetch(
    "https://openidconnect.googleapis.com/v1/userinfo",
    {
      headers: {
        Authorization: `Bearer ${accessToken}`,
      },
    }
  );

  if (!response.ok) {
    throw new Error("Failed to fetch user info");
  }

  const data = await response.json();

  // Log the raw data for debugging
  console.log("Raw Google user data:", JSON.stringify(data));

  return {
    id: data.sub,
    email: data.email,
    name: data.name,
    picture: data.picture,
    locale: data.locale ?? "",
    verifiedEmail: data.verified_email,
  };
}

export async function getDriveInfo(
  accessToken: string
): Promise<GoogleDriveInfo> {
  const oauth2Client = new google.auth.OAuth2();
  oauth2Client.setCredentials({ access_token: accessToken });

  const drive = google.drive({ version: "v3", auth: oauth2Client });

  const response = await drive.about.get({
    fields: "storageQuota",
  });

  const storageQuota = response.data.storageQuota;

  if (!storageQuota) {
    throw new Error("Failed to fetch storage quota");
  }

  const totalBytes = parseInt(storageQuota.limit || "0", 10);
  const usedBytes = parseInt(storageQuota.usage || "0", 10);
  const trashBytes = parseInt(storageQuota.usageInDriveTrash || "0", 10);

  const percentUsed = totalBytes > 0 ? (usedBytes / totalBytes) * 100 : 0;

  return {
    totalStorageBytes: formatBytes(totalBytes),
    usedStorageBytes: formatBytes(usedBytes),
    percentUsed: Math.round(percentUsed * 100) / 100,
    trashBytes: formatBytes(trashBytes),
  };
}

function formatBytes(bytes: number): string {
  if (bytes === 0) return "0 Bytes";

  const k = 1024;
  const sizes = ["Bytes", "KB", "MB", "GB", "TB"];

  const i = Math.floor(Math.log(bytes) / Math.log(k));

  return parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + " " + sizes[i];
}

/**
 * Uploads a file to Google Drive
 * @param accessToken The user's access token
 * @param fileData The blob to upload
 * @param fileName The name of the file
 * @returns The Google Drive file ID and URL
 */
export async function uploadFileToDrive(
  accessToken: string,
  fileData: Blob,
  fileName: string
): Promise<{ id: string; webViewLink: string }> {
  const oauth2Client = new google.auth.OAuth2();
  oauth2Client.setCredentials({ access_token: accessToken });

  const drive = google.drive({ version: "v3", auth: oauth2Client });

  // Convert Blob to Buffer
  const buffer = Buffer.from(await fileData.arrayBuffer());

  // Create a readable stream from the buffer
  const readableStream = new Readable();
  readableStream.push(buffer);
  readableStream.push(null); // Signal the end of the stream

  // Upload file to Google Drive
  const response = await drive.files.create({
    requestBody: {
      name: fileName,
      mimeType: "application/octet-stream",
    },
    media: {
      mimeType: "application/octet-stream",
      body: readableStream, // Use the stream instead of the buffer directly
    },
    fields: "id, webViewLink",
  });

  // Make the file publicly accessible by link
  await drive.permissions.create({
    fileId: response.data.id as string,
    requestBody: {
      role: "reader",
      type: "anyone",
    },
  });

  // Refetch to get the webViewLink
  const file = await drive.files.get({
    fileId: response.data.id as string,
    fields: "id, webViewLink",
  });

  return {
    id: file.data.id as string,
    webViewLink: file.data.webViewLink as string,
  };
}

--- google/googleService.ts ---
/**
 * Google Drive service for client-side file operations
 */
import { DriveInfo, UserInfo } from "@/app/contribution/types";
import { clientSideEncrypt, formatVanaFileId } from "../crypto/utils";

export interface UploadResponse {
  downloadUrl: string;
  fileId: string;
  vanaFileId: string;
}

/**
 * Handle the complete data upload process:
 * 1. Encrypt the data
 * 2. Upload to Google Drive
 * 3. Set permissions
 * 4. Generate and return all necessary URLs
 */
export const uploadUserData = async (
  userInfo: UserInfo,
  signature: string,
  accessToken: string,
  driveInfo?: DriveInfo
): Promise<UploadResponse> => {
  // Prepare data package
  const timestamp = Date.now();
  const dataPackage = {
    userId: userInfo.id || "unknown",
    email: userInfo.email,
    timestamp,
    profile: {
      name: userInfo.name,
      locale: userInfo.locale || "en",
    },
    storage: driveInfo
      ? {
          percentUsed: driveInfo.percentUsed,
        }
      : undefined,
    metadata: {
      source: "Google",
      collectionDate: new Date().toISOString(),
      dataType: "profile",
    },
  };

  const fileString = JSON.stringify(dataPackage);
  const fileBlob = new Blob([fileString], { type: "application/json" });

  // Encrypt the data
  const encryptedBlob = await clientSideEncrypt(fileBlob, signature);

  // Upload to Drive
  const fileName = `vana_dlp_data_${timestamp}.json`;
  const fileDetails = await uploadFileToDrive(
    encryptedBlob,
    fileName,
    accessToken
  );

  // Set permissions and get download URL
  await updateFilePermissions(accessToken, fileDetails.id);
  const downloadUrl = await createSharableLink(fileDetails.id);

  // Return complete response
  return {
    downloadUrl: downloadUrl,
    fileId: fileDetails.id,
    vanaFileId: formatVanaFileId(fileDetails.webViewLink, timestamp),
  };
};

/**
 * Upload a file directly to Google Drive
 * @param file Blob data to upload
 * @param fileName Name for the file
 * @param token Google OAuth access token
 * @returns Object with file details
 */
const uploadFileToDrive = async (
  encryptedBlob: Blob,
  fileName: string,
  token: string
) => {
  const folderId = await findOrCreateFolder(token, "VANA DLP Data");
  const url =
    "https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart";

  // Construct metadata
  const metadata = {
    name: `encrypted_${fileName}`,
    parents: [folderId],
  };

  // Create the multipart body
  const formData = new FormData();
  formData.append(
    "metadata",
    new Blob([JSON.stringify(metadata)], { type: "application/json" })
  );
  formData.append("file", encryptedBlob);

  try {
    const response = await fetch(url, {
      method: "POST",
      headers: {
        Authorization: `Bearer ${token}`,
      },
      body: formData,
    });

    if (!response.ok) {
      throw new Error(
        `Google Drive API error ${response.status}: ${await response.text()}`
      );
    }

    const fileDetails = await response.json();
    return await fetchFileDetails(token, fileDetails.id);
  } catch (error) {
    console.error("Failed to upload file to Google Drive:", error);
    throw error;
  }
};

/**
 * Fetch detailed information about a file
 * @param token Google OAuth access token
 * @param fileId File ID to get details for
 * @returns Object with file details
 */
const fetchFileDetails = async (token: string, fileId: string) => {
  const detailsUrl = `https://www.googleapis.com/drive/v3/files/${fileId}?fields=id,name,webViewLink`;

  const response = await fetch(detailsUrl, {
    method: "GET",
    headers: {
      Authorization: `Bearer ${token}`,
    },
  });

  if (!response.ok) {
    throw new Error(
      `Failed to fetch file details: ${
        response.status
      } ${await response.text()}`
    );
  }

  return await response.json();
};

/**
 * Find or create a folder in Google Drive
 * @param token Google OAuth access token
 * @param folderName Name of the folder to find or create
 * @returns ID of the found or created folder
 */
const findOrCreateFolder = async (token: string, folderName: string) => {
  const url = `https://www.googleapis.com/drive/v3/files`;
  const params = new URLSearchParams({
    q: `name = '${folderName}' and mimeType = 'application/vnd.google-apps.folder' and trashed = false`,
    fields: "files(id, name)",
    pageSize: "1",
  });

  const searchResponse = await fetch(`${url}?${params.toString()}`, {
    method: "GET",
    headers: {
      Authorization: `Bearer ${token}`,
    },
  });

  const data = await searchResponse.json();
  if (data.files && data.files.length > 0) {
    return data.files[0].id; // Return the first found folder's ID
  } else {
    // Folder not found, create it
    const metadata = {
      name: folderName,
      mimeType: "application/vnd.google-apps.folder",
    };
    const createResponse = await fetch(url, {
      method: "POST",
      headers: {
        Authorization: `Bearer ${token}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify(metadata),
    });

    const folderData = await createResponse.json();
    return folderData.id;
  }
};

/**
 * Create a sharable link for a file
 * @param token Google OAuth access token
 * @param fileId ID of the file to create a link for
 * @returns Sharable link for the file
 */
const createSharableLink = async (fileId: string) => {
  return `https://drive.google.com/uc?export=download&id=${fileId}`;
};

/**
 * Update a file's permissions to make it publicly accessible
 * @param token Google OAuth access token
 * @param fileId ID of the file to update permissions for
 */
const updateFilePermissions = async (token: string, fileId: string) => {
  const url = `https://www.googleapis.com/drive/v3/files/${fileId}/permissions`;

  const body = JSON.stringify({
    role: "reader",
    type: "anyone",
  });

  const response = await fetch(url, {
    method: "POST",
    headers: {
      Authorization: `Bearer ${token}`,
      "Content-Type": "application/json",
    },
    body: body,
  });

  if (!response.ok) {
    throw new Error(`Failed to update permissions: ${await response.text()}`);
  }

  return await response.json();
};



#!/bin/bash
echo "----------------------------------------------"
echo "Running Phala Cloud Pre-Launch Script v0.0.2 with additional decryption of encrypted ENVs"
echo "Attempts to decrypt any ENVs with _ENCRYPTED_ENV postfix and export them without the postfix"
echo "Use encrypt_env_var.py to encrypt the variables beforehand against TEE public key"
echo "----------------------------------------------"
set -e

# Function: Perform Docker cleanup
perform_cleanup() {
    echo "Pruning unused images"
    docker image prune -af
    echo "Pruning unused volumes"
    docker volume prune -f
}

# Function: Check Docker login status without exposing credentials
check_docker_login() {
    # Try to verify login status without exposing credentials
    if docker info 2>/dev/null | grep -q "Username"; then
        return 0
    else
        return 1
    fi
}

# Function: Check AWS ECR login status
check_ecr_login() {
    # Check if we can access the registry without exposing credentials
    if aws ecr get-authorization-token --region $DSTACK_AWS_REGION &>/dev/null; then
        return 0
    else
        return 1
    fi
}

# Main logic starts here
echo "Starting login process..."

# Check if Docker credentials exist
if [[ -n "$DSTACK_DOCKER_USERNAME" && -n "$DSTACK_DOCKER_PASSWORD" ]]; then
    echo "Docker credentials found"

    # Check if already logged in
    if check_docker_login; then
        echo "Already logged in to Docker registry"
    else
        echo "Logging in to Docker registry..."
        # Login without exposing password in process list
        if [[ -n "$DSTACK_DOCKER_REGISTRY" ]]; then
            echo "$DSTACK_DOCKER_PASSWORD" | docker login -u "$DSTACK_DOCKER_USERNAME" --password-stdin "$DSTACK_DOCKER_REGISTRY"
        else
            echo "$DSTACK_DOCKER_PASSWORD" | docker login -u "$DSTACK_DOCKER_USERNAME" --password-stdin
        fi

        if [ $? -eq 0 ]; then
            echo "Docker login successful"
        else
            echo "Docker login failed"
            exit 1
        fi
    fi
# Check if AWS ECR credentials exist
elif [[ -n "$DSTACK_AWS_ACCESS_KEY_ID" && -n "$DSTACK_AWS_SECRET_ACCESS_KEY" && -n "$DSTACK_AWS_REGION" && -n "$DSTACK_AWS_ECR_REGISTRY" ]]; then
    echo "AWS ECR credentials found"

    # Check if AWS CLI is installed
    if ! command -v aws &> /dev/null; then
        echo "AWS CLI not installed, installing..."
        curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64-2.24.14.zip" -o "awscliv2.zip"
        echo "6ff031a26df7daebbfa3ccddc9af1450 awscliv2.zip" | md5sum -c
        if [ $? -ne 0 ]; then
            echo "MD5 checksum failed"
            exit 1
        fi
        unzip awscliv2.zip &> /dev/null
        ./aws/install

        # Clean up installation files
        rm -rf awscliv2.zip aws
    else
        echo "AWS CLI is already installed: $(which aws)"
    fi

    # Configure AWS CLI
    aws configure set aws_access_key_id "$DSTACK_AWS_ACCESS_KEY_ID"
    aws configure set aws_secret_access_key "$DSTACK_AWS_SECRET_ACCESS_KEY"
    aws configure set default.region $DSTACK_AWS_REGION
    echo "Logging in to AWS ECR..."
    aws ecr get-login-password --region $DSTACK_AWS_REGION | docker login --username AWS --password-stdin "$DSTACK_AWS_ECR_REGISTRY"
    if [ $? -eq 0 ]; then
        echo "AWS ECR login successful"
    else
        echo "AWS ECR login failed"
        exit 1
    fi
fi

perform_cleanup

echo "----------------------------------------------"
echo "Original Script execution completed"
echo "----------------------------------------------"
echo ""
echo "----------------------------------------------"
echo "Starting Custom Environment Variable Decryption (using Docker + Python)"
echo "----------------------------------------------"

# --- Custom Decryption Logic ---

# Configuration
APP_KEYS_FILE_ON_HOST="/tapp/appkeys.json" # Path on the CVM host OS
APP_KEYS_FILE_IN_CONTAINER="/tmp/appkeys.json" # Path inside the temporary container
ENCRYPTED_SUFFIX="_ENCRYPTED_ENV" # Suffix to identify encrypted variables
PYTHON_IMAGE="python:3.11-slim" # Or python:3.11-alpine if size is critical

# 1. Check prerequisites
echo "Checking prerequisites..."
if ! command -v docker &> /dev/null; then
    echo "Error: 'docker' command not found. Cannot perform decryption."
    exit 1
fi
if [ ! -f "$APP_KEYS_FILE_ON_HOST" ]; then
    echo "Error: App keys file not found at '$APP_KEYS_FILE_ON_HOST'."
    exit 1
fi

# Find environment variables ending with the specified suffix
# Find environment variables ending with the specified suffix using 'env' command
ENCRYPTED_VAR_NAMES=$(env | grep "${ENCRYPTED_SUFFIX}=" | cut -d= -f1)

if [ -z "$ENCRYPTED_VAR_NAMES" ]; then
    echo "Info: No environment variables ending in '$ENCRYPTED_SUFFIX' found. Skipping decryption."
    echo "----------------------------------------------"
    echo "Custom Decryption Skipped"
    echo "----------------------------------------------"
else
    # Preparing Python command, ensuring shim executable, the loop, etc.

echo "Found encrypted variables to process:"
echo "$ENCRYPTED_VAR_NAMES"
echo "---"

    # 2. Prepare the inline Python script for the container
PYTHON_DECRYPT_COMMAND=$(cat <<'EOF'
import sys, os, json, binascii
from cryptography.hazmat.primitives.ciphers.aead import AESGCM
from cryptography.hazmat.primitives.asymmetric import x25519

APP_KEYS_FILE = os.environ.get('APP_KEYS_FILE_IN_CONTAINER', '/tmp/appkeys.json')
ENCRYPTED_HEX = os.environ.get('ENCRYPTED_HEX_VALUE', '')

if not ENCRYPTED_HEX:
    print("Error: ENCRYPTED_HEX_VALUE env var not set inside container.", file=sys.stderr)
    sys.exit(1)

def read_priv_key(path):
    try:
        with open(path, 'r') as f: data = json.load(f)
        hex_key = data.get("env_crypt_key")
        if not hex_key: raise ValueError("env_crypt_key not found")
        return binascii.unhexlify(hex_key)
    except Exception as e: raise ValueError(f"Failed to read key from {path}: {e}")

def decrypt(encrypted_hex, priv_key_bytes):
    try:
        encrypted_data = binascii.unhexlify(encrypted_hex)
        if len(encrypted_data) < 44: raise ValueError("Encrypted data too short")
        eph_pub_bytes = encrypted_data[:32]
        iv = encrypted_data[32:44]
        ciphertext_tag = encrypted_data[44:] # Keep tag appended

        priv_key = x25519.X25519PrivateKey.from_private_bytes(priv_key_bytes)
        eph_pub_key = x25519.X25519PublicKey.from_public_bytes(eph_pub_bytes)
        shared = priv_key.exchange(eph_pub_key)

        aesgcm = AESGCM(shared)
        # Decrypt directly to bytes
        decrypted_bytes = aesgcm.decrypt(iv, ciphertext_tag, None)
        # Decode the bytes as UTF-8 string
        return decrypted_bytes.decode('utf-8')
    except Exception as e:
        # Provide more specific error context if possible
        raise RuntimeError(f"Decryption failed: {e}")

try:
    priv = read_priv_key(APP_KEYS_FILE)
    decrypted_string = decrypt(ENCRYPTED_HEX, priv)
    print(decrypted_string, end='') # Print the raw decrypted string to stdout
except Exception as e:
    print(f"Python Error: {e}", file=sys.stderr)
    sys.exit(1)
EOF
)

# Escape the python script properly for embedding within the sh -c '...' command
PYTHON_DECRYPT_COMMAND_ESCAPED=$(echo "$PYTHON_DECRYPT_COMMAND" | sed "s/'/'\\\\''/g")

# 3. Ensure Docker shim is executable BEFORE running the container
echo "Ensuring Docker shim is executable..."
if [ -f /usr/bin/containerd-shim-runc-v2 ]; then
    chmod +x /usr/bin/containerd-shim-runc-v2
else
    echo "Warning: Shim /usr/bin/containerd-shim-runc-v2 not found. Docker might fail."
fi

# Loop through each encrypted variable name
DECRYPTION_ERRORS=0
for encrypted_var_name in $ENCRYPTED_VAR_NAMES; do
    echo "--- Processing variable: $encrypted_var_name ---"

    # Get the encrypted value using indirect expansion
    encrypted_hex_value="${!encrypted_var_name}"

    if [ -z "$encrypted_hex_value" ]; then
        echo "Warning: Variable '$encrypted_var_name' is set but empty. Skipping."
        continue
    fi

    # Calculate the target decrypted variable name by removing the suffix
    decrypted_var_name="${encrypted_var_name%$ENCRYPTED_SUFFIX}"

    echo "Target decrypted variable name: $decrypted_var_name"

    # 4. Run the temporary Docker container to perform decryption for this variable
    echo "Running decryption container ($PYTHON_IMAGE) for $encrypted_var_name..."
    # Make sure pip install doesn't output progress to interfere with capturing the final print
    DECRYPTED_VALUE=$(docker run --rm \
        -e ENCRYPTED_HEX_VALUE="$encrypted_hex_value" \
        -e APP_KEYS_FILE_IN_CONTAINER="$APP_KEYS_FILE_IN_CONTAINER" \
        -v "$APP_KEYS_FILE_ON_HOST":"$APP_KEYS_FILE_IN_CONTAINER":ro \
        "$PYTHON_IMAGE" \
        sh -c "pip install cryptography --quiet --disable-pip-version-check --no-cache-dir > /dev/null && python -c '$PYTHON_DECRYPT_COMMAND_ESCAPED'"
    )
    DOCKER_EXIT_CODE=$?

    # 5. Check decryption result and export the decrypted variable
    if [ $DOCKER_EXIT_CODE -ne 0 ]; then
        echo "Error: Docker decryption container failed for '$encrypted_var_name' (exit code $DOCKER_EXIT_CODE)."
        DECRYPTION_ERRORS=$((DECRYPTION_ERRORS + 1))
        continue # Continue to the next variable
    fi

    # Check if the result is empty OR contains newline characters (which might indicate an error message)
    # We expect a single string value here.
    if [[ -z "$DECRYPTED_VALUE" ]] || [[ "$DECRYPTED_VALUE" == *$'\n'* ]]; then
        echo "Error: Decryption container returned an empty or multi-line value for '$encrypted_var_name'. Check container logs if possible."
        echo "Returned value was: '$DECRYPTED_VALUE'" # Show what was returned
        DECRYPTION_ERRORS=$((DECRYPTION_ERRORS + 1))
        continue # Continue to the next variable
    fi

    echo "Decryption successful for $encrypted_var_name."
    # Export the decrypted value under the new variable name
    export "$decrypted_var_name"="$DECRYPTED_VALUE"
    echo "Exported $decrypted_var_name for subsequent processes."
    # Debug only: print masked value for confirmation
    # echo "Decrypted value (masked): ${DECRYPTED_VALUE:0:3}..."

done # End of loop through encrypted variables

echo "---"

# Final status check
if [ $DECRYPTION_ERRORS -gt 0 ]; then
    echo "----------------------------------------------"
    echo "Custom Decryption Completed with $DECRYPTION_ERRORS ERRORS."
    echo "----------------------------------------------"
    exit 1 # Exit with error if any decryption failed
else
    echo "----------------------------------------------"
    echo "Custom Decryption Completed Successfully for all variables."
    echo "----------------------------------------------"
fi

fi # End of the main if/else block

# Script implicitly exits with 0 here if the 'else' block was skipped
# or if the 'else' block ran and DECRYPTION_ERRORS was 0.

# --- End of Custom Decryption Logic ---

# The Docker Compose command (e.g., docker compose up -d) would follow this script
# in the CVM's actual boot sequence, inheriting the exported variables.


import os
from google.cloud import kms
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import padding
from cryptography.hazmat.primitives import hashes

kms_client = kms.KeyManagementServiceClient()

def get_key_name():
    project_id = os.environ.get('GCP_PROJECT_ID')
    location_id = os.environ.get('GCP_KMS_LOCATION')
    key_ring_id = os.environ.get('GCP_KMS_KEYRING')
    key_id = os.environ.get('GCP_KMS_KEY_ID')
    key_version = os.environ.get('GCP_KMS_KEY_VERSION')
    return f"{kms_client.crypto_key_path(project_id, location_id, key_ring_id, key_id)}/cryptoKeyVersions/{key_version}"

def get_public_key():
    public_key = kms_client.get_public_key(request={'name': get_key_name()})
    return serialization.load_pem_public_key(public_key.pem.encode())

def encrypt_value(value: str) -> str:
    public_key = get_public_key()
    encrypted = public_key.encrypt(
        value.encode('utf-8'),
        padding.OAEP(
            mgf=padding.MGF1(algorithm=hashes.SHA256()),
            algorithm=hashes.SHA256(),
            label=None
        )
    )
    return encrypted.hex()

def decrypt_value(encrypted: str) -> str:
    ciphertext = bytes.fromhex(encrypted)
    decrypt_response = kms_client.asymmetric_decrypt(
        request={
            "name":  get_key_name(),
            "ciphertext": ciphertext,
        }
    )
    return decrypt_response.plaintext.decode('utf-8')




# Decrypt an encrypted key using a private key
import hashlib
import hmac
from cryptography.hazmat.primitives.asymmetric import ec
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.backends import default_backend
import binascii


def decrypt_with_private_key(encrypted_data: str, private_key: str) -> str:
    # Remove '0x' prefix if present
    private_key = private_key[2:] if private_key.startswith("0x") else private_key
    encrypted_data = encrypted_data[2:] if encrypted_data.startswith("0x") else encrypted_data

    # Convert hex strings to bytes
    private_key_bytes = binascii.unhexlify(private_key)
    encrypted_data_bytes = binascii.unhexlify(encrypted_data)

    # Parse the encrypted data
    iv = encrypted_data_bytes[:16]
    ephemPublicKey = encrypted_data_bytes[16:81]
    ciphertext = encrypted_data_bytes[81:-32]
    mac = encrypted_data_bytes[-32:]

    # Load the private key
    private_key = ec.derive_private_key(
        int.from_bytes(private_key_bytes, byteorder='big'),
        ec.SECP256K1(),
        default_backend()
    )

    # Load the ephemeral public key
    ephemeral_public_key = ec.EllipticCurvePublicKey.from_encoded_point(
        ec.SECP256K1(),
        ephemPublicKey
    )

    # Perform ECDH to get the shared secret
    shared_key = private_key.exchange(ec.ECDH(), ephemeral_public_key)

    # Derive encryption and MAC keys
    hash_key = hashlib.sha512(shared_key).digest()
    enc_key = hash_key[:32]
    mac_key = hash_key[32:]

    # Verify MAC
    dataToMac = iv + ephemPublicKey + ciphertext
    calculated_mac = hmac.new(mac_key, dataToMac, hashlib.sha256).digest()
    if not hmac.compare_digest(calculated_mac, mac):
        raise ValueError("Invalid MAC")

    # Decrypt
    cipher = Cipher(algorithms.AES(enc_key), modes.CBC(iv), backend=default_backend())
    decryptor = cipher.decryptor()
    decrypted = decryptor.update(ciphertext) + decryptor.finalize()

    # Remove PKCS7 padding
    padding_length = decrypted[-1]
    decrypted = decrypted[:-padding_length]

    return decrypted.decode('utf-8')


# Example usage
encrypted_data = "d22c0ec2139e0e5b30af989cf320ec7f0455732bd1776f8249992d00b21ae0c12724448cff74e6b9ab7f92179d25ccb302f74224724921a1b37540d15200a24f72ac6da53ea677a6a4d5a3bb434cce1558332eb0fdfefed99d7e7129fca8f96ea59b5ab75f51d8d2d9fafa3e6aa3e700b1caea99e9d9b4e11ec778ce07fd77b8fd5e383654c68eab8a5dec665ab4616c3f07178291c1893d3ca24628921384ada76c6bff3705f24c40651587db3ee3eb777b164666a49aee656cbf6c0ae25c5426447c7d61db305c75b56e38e6aab154e5c99ec97955b2cf002963ca67dee70b43183655d12d201f1f48292c03eda22178d0d6cf6ed1dccd7586088e07493f257f"
private_key = "enter_your_private_key"

try:
    decrypted_text = decrypt_with_private_key(encrypted_data, private_key)
    print("Decrypted text:", decrypted_text)
except Exception as e:
    print("Decryption failed:", str(e))


#!/usr/bin/env python3
import os
import json
import secrets
from typing import List, Dict, Any, Optional
import asyncio

import httpx
from dotenv import load_dotenv
from cryptography.hazmat.primitives.asymmetric import x25519
from cryptography.hazmat.primitives.ciphers.aead import AESGCM
import argparse
import os

# Initialize environment
load_dotenv()

# API client setup
class PhalaCVMClient:
    def __init__(
            self,
            base_url: str = "https://cloud-api.phala.network/api/v1",
            timeout: float = 60.0
    ):
        self.base_url = base_url
        self.client = httpx.Client(
            base_url=base_url,
            headers={
                'Content-Type': 'application/json',
                'x-api-key': os.getenv('PHALA_CLOUD_API_KEY'),
            },
            timeout=timeout
        )

    def get_pubkey(self, vm_config: Dict[str, Any]) -> Dict[str, str]:
        response = self.client.post("/cvms/pubkey/from_cvm_configuration", json=vm_config)
        response.raise_for_status()
        return response.json()

    def create_vm(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Create a VM in Phala Cloud with detailed error logging."""
        print("Sending VM creation request to Phala Cloud...")
        response = self.client.post("/cvms/from_cvm_configuration", json=config)

        try:
            response.raise_for_status()
            return response.json()
        except httpx.HTTPStatusError as e:
            print(f"HTTP Error Status: {e.response.status_code}")
            print(f"Full error response: {e.response.text}")

            # Try to parse the error response as JSON for more details
            try:
                error_details = e.response.json()
                print(f"Error details: {json.dumps(error_details, indent=2)}")
            except:
                print("Error response is not valid JSON")
            raise

    def list_vms(self) -> List[Dict[str, Any]]:
        response = self.client.get("/cvms")
        response.raise_for_status()
        return response.json()

    def get_vm_details(self, vm_id: str) -> Dict[str, Any]:
        """Get details of a specific VM."""
        response = self.client.get(f"/cvms/{vm_id}")
        response.raise_for_status()
        return response.json()

    def get_vm_compose(self, vm_id: str) -> Dict[str, Any]:
        """Get the compose manifest of a VM."""
        response = self.client.get(f"/cvms/{vm_id}/compose")
        response.raise_for_status()
        return response.json()

    def update_vm_compose(
            self,
            vm_id: str,
            compose_manifest: Dict[str, Any],
            encrypted_env: Optional[str] = None,
            timeout: float = 120.0
    ) -> Dict[str, Any]:
        """Update a VM's compose manifest"""
        body = {"compose_manifest": compose_manifest}
        if encrypted_env:
            body["encrypted_env"] = encrypted_env

        print(f"Sending update request for VM {vm_id}...")

        # Print body
        print("Request body for update:")
        print(json.dumps(body, indent=2))

        print("public_log: ", body['compose_manifest'].get("public_logs"))

        try:
            response = self.client.patch(
                f"/cvms/{vm_id}/compose",
                json=body,
                timeout=timeout
            )
            response.raise_for_status()
            print(f"VM update request completed successfully")
            return response.json()
        except httpx.ReadTimeout:
            # If we get a timeout, check if the update was successful
            print("Update request timed out but may have been accepted by the server.")
            print(f"Checking status of VM {vm_id}...")

            # Wait a moment before checking status
            import time
            time.sleep(2)

            # Check VM status
            try:
                vm_details = self.get_vm_details(vm_id)
                if vm_details.get("status") in ["Running", "Updating", "Accepted"]:
                    print(f"VM status is '{vm_details.get('status')}'. The update appears to be in progress or completed.")
                    return {"status": "Accepted", "message": "Update request accepted", "vm_id": vm_id}
                else:
                    print(f"VM has status: {vm_details.get('status')}. The update may not have been processed.")
            except Exception as check_error:
                print(f"Error checking VM status after timeout: {str(check_error)}")

            # Return a custom response with enough information to continue
            return {
                "status": "Unknown",
                "message": "Update request timed out but may have been accepted. Check VM status manually.",
                "vm_id": vm_id
            }
        except httpx.HTTPStatusError as e:
            print(f"HTTP Error Status: {e.response.status_code}")
            print(f"Full error response: {e.response.text}")

            # Try to parse the error response as JSON for more details
            try:
                error_details = e.response.json()
                print(f"Error details: {json.dumps(error_details, indent=2)}")
            except:
                print("Error response is not valid JSON")
            raise

    def get_available_teepods(self) -> Dict[str, Any]:
        """Get list of available Teepods from Phala Cloud."""
        print("Requesting available Teepods from Phala Cloud...")
        response = self.client.get("/teepods/available")
        response.raise_for_status()
        return response.json()

# Helper functions
def encrypt_env_vars(envs: List[Dict[str, str]], public_key_hex: str) -> str:
    """
    Encrypt environment variables for Phala Cloud.

    Args:
        envs: List of environment variables to encrypt
        public_key_hex: Public key hex string for encryption

    Returns:
        Hex string of encrypted environment variables
    """
    # Convert environment variables to JSON
    envs_json = json.dumps({"env": envs})

    # Generate private key and get public key
    private_key = x25519.X25519PrivateKey.generate()
    public_key = private_key.public_key()
    my_public_bytes = public_key.public_bytes_raw()

    # Convert remote public key from hex and create public key object
    remote_public_key_bytes = bytes.fromhex(public_key_hex.replace("0x", ""))
    remote_public_key = x25519.X25519PublicKey.from_public_bytes(remote_public_key_bytes)

    # Generate shared key
    shared_key = private_key.exchange(remote_public_key)

    # Generate random IV (12 bytes for AES-GCM)
    iv = secrets.token_bytes(12)

    # Encrypt data
    aesgcm = AESGCM(shared_key)
    encrypted_data = aesgcm.encrypt(iv, envs_json.encode(), None)

    # Combine all components: sender public key + IV + ciphertext
    result = my_public_bytes + iv + encrypted_data
    return result.hex()

def read_docker_compose(file_path: str, docker_tag: str = "latest") -> str:
    """Read Docker Compose file and replace variables."""
    try:
        with open(file_path, 'r') as f:
            content = f.read()

        # Replace tag variable if present
        content = content.replace('${DOCKER_TAG}', docker_tag)

        return content
    except FileNotFoundError:
        raise FileNotFoundError(f"Docker Compose file not found at {file_path}. Please create this file before running the deployment.")

def read_pre_launch_script(file_path: str) -> str:
    """Read pre-launch script file."""
    try:
        with open(file_path, 'r') as f:
            content = f.read()
        return content
    except FileNotFoundError:
        raise FileNotFoundError(f"Pre-launch script file not found at {file_path}. Please create this file before running the deployment.")

async def deploy(
        teepod_id: int,
        image: str,
        vm_name: str,
        vm_id: Optional[str] = None,
        docker_compose_file: str = "docker-compose-tee-phala-cloud.yml",
        docker_tag: str = "latest",
        update_existing: bool = True, # Keep this flag to control update behavior when vm_id is provided
        timeout: float = 120.0,
        vcpu: int = 2,
        memory: int = 8192,
        disk_size: int = 40,
        env_vars_to_encrypt: List[Dict[str, str]] = None,
) -> Dict[str, Any]:
    """Deploy a VM to Phala Cloud or update an existing one if vm_id is provided."""

    # Read Docker Compose configuration from file
    docker_compose = read_docker_compose(docker_compose_file, docker_tag)
    pre_launch_script = read_pre_launch_script("prelaunch.sh")
    print(f"Using Docker tag: {docker_tag}")

    vm_config = {
        "name": vm_name,
        "compose_manifest": {
            "name": vm_name,
            "docker_compose_file": docker_compose,
            "pre_launch_script": pre_launch_script,
        },
        "vcpu": vcpu,
        "memory": memory,
        "disk_size": disk_size,
        "teepod_id": teepod_id,
        "image": image,
        "advanced_features": {
            "tproxy": True,
            "kms": True,
            "docker_config": {
                "password": "",
                "registry": None,
                "username": "",
            },
            "listed": False,
        }
    }

    # Check for required API key
    if not os.getenv("PHALA_CLOUD_API_KEY"):
        raise ValueError("Missing required environment variable: PHALA_CLOUD_API_KEY")

    # Create client with the specified timeout
    client = PhalaCVMClient(timeout=timeout)

    # If a specific VM ID is provided, update it
    if vm_id:
        if update_existing:
            print(f"Updating VM with ID {vm_id}...")
            # Get the existing VM's compose settings to retrieve its public key
            try:
                vm_compose = client.get_vm_compose(vm_id)
            except httpx.HTTPStatusError as e:
                print(f"Failed to get compose details for VM {vm_id}: {e}")
                raise ValueError(f"Could not retrieve details for VM ID {vm_id}. Ensure it exists and is accessible.")

            # Create a properly structured compose manifest for the update
            compose_manifest = {
                "name": vm_name, # Allow name update if needed
                "docker_compose_file": docker_compose,
                "pre_launch_script": pre_launch_script,
            }

            # Encrypt environment variables using the existing VM's pubkey
            encrypted_env = None
            if env_vars_to_encrypt:
                encrypted_env = encrypt_env_vars(
                    env_vars_to_encrypt,
                    vm_compose["env_pubkey"],
                )

            print("Manifest for update:")
            print(json.dumps(compose_manifest, indent=2))

            # Update the VM with the longer timeout
            response = client.update_vm_compose(
                vm_id,
                compose_manifest,
                encrypted_env,
                timeout=timeout
            )

            # if response.get("status") in ["Accepted", "Unknown"]: # Handle timeout case
            #     print(f"VM with ID {vm_id} update initiated successfully (status: {response.get('status')}).")
            # else:
            #     print(f"VM update failed with status: {response.get('status')}")
            #     print(json.dumps(response, indent=2))
            return response
        else:
            # If vm_id is provided but update is disabled, treat it as a skip
            print(f"VM ID {vm_id} provided, but update flag is not set. Skipping.")
            return {"status": "skipped", "message": f"Update skipped for VM {vm_id}", "id": vm_id}

    # If no vm_id is provided, create a new VM
    print(f"Creating new VM {vm_name}...")

    # Step 1: Get encryption public key for the new VM config
    with_pubkey = client.get_pubkey(vm_config)

    # Step 2: Encrypt environment variables if provided
    encrypted_env = None
    if env_vars_to_encrypt:
        encrypted_env = encrypt_env_vars(
            env_vars_to_encrypt,
            with_pubkey["app_env_encrypt_pubkey"],
        )

    # Step 3: Create VM with encrypted environment variables
    create_payload = {
        **vm_config,
        "app_env_encrypt_pubkey": with_pubkey["app_env_encrypt_pubkey"],
        "app_id_salt": with_pubkey["app_id_salt"],
    }
    if encrypted_env:
        create_payload["encrypted_env"] = encrypted_env

    print("Manifest for creation:")
    print(json.dumps(create_payload, indent=2))

    response = client.create_vm(create_payload)
    print(f"VM {vm_name} creation initiated.")
    return response

async def main():
    parser = argparse.ArgumentParser(description='Deploy or update a VM on Phala Cloud')
    parser.add_argument('--teepod-id', type=int, default=3, help='Teepod ID to deploy to (default: 3)')
    parser.add_argument('--image', type=str, default="dstack-dev-0.3.5", help='Phala VM image name (default: dstack-dev-0.3.5)')
    parser.add_argument('--vm-name', type=str, help='Name for the VM (defaults to env var PHALA_VM_NAME or "test-api")')
    parser.add_argument('--vm-id', type=str, help='Specific VM ID to update (if provided, updates this VM; otherwise creates a new one; defaults to env var PHALA_VM_ID)')
    parser.add_argument('--update', action='store_true', help='Allow updating if --vm-id is provided (default: False, unless --vm-id is set)')
    parser.add_argument('--docker-tag', type=str, default="latest", help='Docker image tag to use in compose file (default: latest)')
    parser.add_argument('--docker-compose-file', type=str, default="docker-compose-tee-phala-cloud.yml",
                        help='Path to Docker Compose file (default: docker-compose-tee-phala-cloud.yml)')
    parser.add_argument('--list-teepods', action='store_true', help='List available Teepods and exit')
    parser.add_argument('--list-vms', action='store_true', help='List all VMs and exit')
    parser.add_argument('--timeout', type=float, default=120.0, help='Timeout in seconds for API requests (default: 120)')
    parser.add_argument('--vcpu', type=int, default=2, help='Number of virtual CPUs (default: 2)')
    parser.add_argument('--memory', type=int, default=8192, help='Memory in MB (default: 8192)')
    parser.add_argument('--disk-size', type=int, default=40, help='Disk size in GB (default: 40)')
    parser.add_argument('--env-file', type=str, help='Path to .env file containing environment variables to encrypt')
    parser.add_argument('--env', action='append', help='Environment variable to encrypt (KEY=VALUE format)', default=[])
    parser.add_argument('--auto-env', action='store_true', default=True, help='Automatically encrypt all environment variables (default: True)')
    parser.add_argument('--include-env', action='append', help='Explicitly include these environment variables (if auto-env is used)', default=[])
    parser.add_argument('--exclude-env', action='append', help='Exclude these environment variables (if auto-env is used)', default=[])
    parser.add_argument('--list-env', action='store_true', help='List environment variables that would be encrypted and exit')

    args = parser.parse_args()

    # Create client with the specified timeout
    client = PhalaCVMClient(timeout=args.timeout)

    # Option to just list Teepods and exit
    if args.list_teepods:
        teepods = client.get_available_teepods()
        print("Available Teepods:")
        print(json.dumps(teepods, indent=2))
        return teepods

    # Option to list all VMs and exit
    if args.list_vms:
        vms = client.list_vms()
        print("Available VMs:")
        if not vms:
            print("No VMs found.")
        else:
            for vm in vms:
                print(f"ID: {vm.get('id')} | Name: {vm.get('name')} | Status: {vm.get('status')}")
        return vms

    # Parse environment variables to encrypt
    env_vars_to_encrypt = []
    env_keys_added = set() # Keep track of keys to avoid duplicates

    def add_env_var(key, value):
        if key not in env_keys_added:
            env_vars_to_encrypt.append({"key": key, "value": value})
            env_keys_added.add(key)

    # Add variables from --env arguments first (higher priority)
    for env_str in args.env:
        if '=' in env_str:
            key, value = env_str.split('=', 1)
            add_env_var(key, value)
        else:
            print(f"Warning: Skipping incorrectly formatted --env variable: {env_str}")

    # Add variables from env file if specified
    if args.env_file:
        try:
            with open(args.env_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        add_env_var(key, value)
        except Exception as e:
            print(f"Error reading environment file {args.env_file}: {str(e)}")
            raise

    # Automatically use system environment variables if auto-env is set
    if args.auto_env:
        print("Automatically collecting environment variables...")
        # Exclude common system variables and CI-related variables
        default_exclusions = {
            'PATH', 'HOME', 'USER', 'SHELL', 'PWD', 'OLDPWD', 'TERM',
            'SHLVL', 'HOSTNAME', 'PHALA_CLOUD_API_KEY', '_',
            'PYTHONPATH', 'LANG', 'LC_ALL', 'LESSOPEN', 'LESSCLOSE',
            'CI', 'GITHUB_ACTIONS', 'RUNNER_OS', 'RUNNER_ARCH', 'RUNNER_NAME',
            'AGENT_OS', 'AGENT_MACHINETYPE', 'AGENT_VERSION',
            'SYSTEM_TEAMFOUNDATIONCOLLECTIONURI', 'SYSTEM_TEAMPROJECT',
            'BUILD_BUILDID', 'BUILD_BUILDNUMBER', 'BUILD_REPOSITORY_NAME',
            'BUILD_SOURCEBRANCHNAME', 'BUILD_REASON',
        }
        exclusions = default_exclusions.union(set(args.exclude_env))
        env_dict = dict(os.environ)
        included_vars_auto = []

        if args.include_env:
            print(f"Including only specifically requested variables via --include-env: {args.include_env}")
            for key in args.include_env:
                if key in env_dict and key not in exclusions:
                    add_env_var(key, env_dict[key])
                    included_vars_auto.append(key)
        else:
            for key, value in env_dict.items():
                # Skip exclusions and common CI/runner prefixes
                if key not in exclusions and not key.startswith(('GITHUB_', 'RUNNER_', 'AGENT_', 'SYSTEM_', 'BUILD_')):
                    add_env_var(key, value)
                    included_vars_auto.append(key)

        print(f"Automatically included {len(included_vars_auto)} environment variables (after exclusions and explicit includes/overrides).")

        if args.list_env: # Show auto-included vars if listing
            print("Variables automatically included (after filtering):")
            for var in included_vars_auto:
                value = env_dict[var]
                display_value = value[:5] + "..." + value[-2:] if len(value) > 10 else value[:2] + "***"
                print(f"  {var}={display_value}")

    # Option to just list environment variables and exit
    if args.list_env:
        print(f"\nFinal list of environment variables to be encrypted ({len(env_vars_to_encrypt)} total):")
        if not env_vars_to_encrypt:
            print("  (None)")
        else:
            for item in env_vars_to_encrypt:
                key = item['key']
                value = item['value']
                display_value = value[:5] + "..." + value[-2:] if len(value) > 10 else value[:2] + "***"
                print(f"  {key}={display_value}")
        return {"status": "list_env_only", "env_count": len(env_vars_to_encrypt)}

    # Print final count of environment variables being encrypted
    print(f"Will encrypt {len(env_vars_to_encrypt)} environment variables for the VM.")

    try:
        # Get VM name from args or environment variable
        vm_name = args.vm_name if args.vm_name else os.getenv('PHALA_VM_NAME', 'test-api')
        # Get VM ID from args or environment variable
        vm_id = args.vm_id if args.vm_id else os.getenv('PHALA_VM_ID')

        # Note:
        # If vm_id is provided, --update is implicitly True unless explicitly denied
        # If vm_id is not provided, --update has no effect
        update_flag = args.update or bool(vm_id)

        print(f"Target VM Name: {vm_name}")
        if vm_id:
            print(f"Target VM ID: {vm_id} (implies update)")
        else:
            print("No VM ID provided, will create a new VM.")

        # Proceed with deployment or update
        response = await deploy(
            teepod_id=args.teepod_id,
            image=args.image,
            vm_name=vm_name,
            vm_id=vm_id,
            docker_compose_file=args.docker_compose_file,
            docker_tag=args.docker_tag,
            update_existing=update_flag, # Use derived update flag
            timeout=args.timeout,
            vcpu=args.vcpu,
            memory=args.memory,
            disk_size=args.disk_size,
            env_vars_to_encrypt=env_vars_to_encrypt,
        )

        # if response.get("status") == "Unknown":
        #     print('\nOperation completed with timeout, but may have succeeded:')
        #     print(json.dumps(response, indent=2))
        #     print('Please check the VM status in Phala Cloud UI')
        # elif response.get("status") == "skipped":
        #     print('\nOperation skipped:')
        #     print(json.dumps(response, indent=2))
        # else:
        #     print('\nOperation successful or initiated:', json.dumps(response, indent=2))

        return response
    except Exception as error:
        import traceback
        print('\nOperation failed:', str(error))
        # print(traceback.format_exc()) # Uncomment for full traceback if needed
        raise

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception:
        # Catch exception raised from main to prevent non-zero exit code unless needed
        # print("Exiting due to error.") # Error is already printed in main's exception handler
        exit(1) # Exit with error code if main fails

#!/bin/bash

set -e

# Function to check if a command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Check if GPG is installed
if ! command_exists gpg; then
    echo "Error: GPG is not installed on your system."
    echo "Please install GPG and try again."
    echo ""
    echo "Installation instructions:"
    echo "- On macOS (using Homebrew): brew install gnupg"
    echo "- On Ubuntu/Debian: sudo apt-get update && sudo apt-get install gnupg"
    echo "- On other systems, please refer to your package manager or https://gnupg.org"
    exit 1
fi

# Function to prompt for input with a default value
prompt_with_default() {
    local prompt="$1"
    local default="$2"
    local response

    read -p "$prompt [$default]: " response
    echo "${response:-$default}"
}

# Detect operating system
if [[ "$OSTYPE" == "darwin"* ]]; then
    OS="macOS"
elif [[ "$OSTYPE" == "linux-gnu"* ]]; then
    OS="Linux"
else
    echo "Unsupported operating system: $OSTYPE"
    exit 1
fi

# Function to create a temporary directory
create_temp_dir() {
    if [[ "$OS" == "macOS" ]]; then
        mktemp -d -t gpg_temp
    else
        mktemp -d
    fi
}

# Function to base64 encode
base64_encode() {
    if [[ "$OS" == "macOS" ]]; then
        base64 -i "$1" -o "$2"
    else
        base64 -w 0 "$1" > "$2"
    fi
}

# Prompt for user information
NAME=$(prompt_with_default "Enter your name" "Vana DLP Validator")
EMAIL=$(prompt_with_default "Enter your email" "validator@example.com")
EXPIRE=$(prompt_with_default "Enter key expiration in days (0 for no expiration)" "0")

# Create a temporary GPG home directory
GNUPGHOME=$(create_temp_dir)
export GNUPGHOME
trap 'rm -rf "$GNUPGHOME"' EXIT

# Generate GPG key
echo "Generating GPG key..."
gpg --batch --gen-key <<EOF
%echo Generating a basic OpenPGP key
Key-Type: RSA
Key-Length: 3072
Subkey-Type: RSA
Subkey-Length: 3072
Name-Real: $NAME
Name-Email: $EMAIL
Expire-Date: $EXPIRE
%no-protection
%commit
%echo done
EOF

# Export public key
echo "Exporting public key..."
gpg --armor --export "$EMAIL" > public_key.asc

# Export private key
echo "Exporting private key..."
gpg --armor --export-secret-keys "$EMAIL" > private_key.asc

# Check if the files were created and have content
if [ ! -s public_key.asc ]; then
    echo "Error: public_key.asc is empty or was not created."
    exit 1
fi

if [ ! -s private_key.asc ]; then
    echo "Error: private_key.asc is empty or was not created."
    exit 1
fi

# Base64 encode public key
echo "Base64 encoding public key..."
base64_encode public_key.asc public_key_base64.asc

# Base64 encode private key
echo "Base64 encoding private key..."
base64_encode private_key.asc private_key_base64.asc

echo "Done. Keys have been generated and exported."
echo "Public key: public_key.asc"
echo "Private key: private_key.asc"
echo "Base64 encoded public key: public_key_base64.asc"
echo "Base64 encoded private key: private_key_base64.asc"

echo "Please copy the contents of private_key_base64.asc to your .env file under PRIVATE_FILE_ENCRYPTION_PUBLIC_KEY_BASE64"

# Display the contents of public_key.asc
echo "Contents of public_key.asc:"
cat public_key.asc

# Notes on managing keys and encryption

## Generating a key pair

This is used by the client-side encryption function to encrypt files and by DLP validators to decrypt files encrypted by the client-side encryption function.

```shell
gpg --full-generate-key
```

This will prompt you to select the type of key you want to generate.
- Select `RSA and RSA` (option 1) and then select the key size you want to generate.
- Recommended key size is `3072` bits.
- After that, you will be prompted to enter your name and email address.
- You can leave the comment field empty.
- After that, you will be prompted to enter a passphrase to protect your private key.
- After that, GPG will generate a lot of random bytes to generate the key pair.


## Backup key to file
```shell
gpg --armor --export-secret-keys your-email@example.com > my-private-key.asc
```

Use the following command to list the keys you have generated with details:

```shell
gpg --list-keys --keyid-format LONG
```

This will display a list of all the keys you have along with details such as the key IDs, creation dates, and associated emails.
Look for the key you created most recently.

- Identify the Key ID
   From the output, identify the key ID of the latest key. The key ID is usually displayed next to the 'pub' keyword. It will look something like this: rsa4096/1234ABCD1234ABCD 2023-01-01 [SC].

- Export the Specific Key
   Once you have identified the correct key ID, you can export just that key by replacing your-email@example.com with the key ID in the export command. For example:

```bash
gpg --armor --export 1234ABCD1234ABCD > publickey.asc
```

## Convert the keys to base64

```shell
base64 -i publickey.asc -o publickey_base64.asc
base64 -i privatekey.asc -o privatekey_base64.asc
```

## Decrypting a file

```shell
gpg --output decrypted_image.png --decrypt encrypted_image.png
```

## Import a key

Used to import a symmetric key generated by the client-side encryption function in the UI.

```shell
gpg --import decrypted_symmetric_key.asc
```

# Notes on managing keys and encryption

## Generating a key pair

This is used by the client-side encryption function to encrypt files and by DLP validators to decrypt files encrypted by the client-side encryption function.

```shell
gpg --full-generate-key
```

This will prompt you to select the type of key you want to generate.
- Select `RSA and RSA` (option 1) and then select the key size you want to generate.
- Recommended key size is `3072` bits.
- After that, you will be prompted to enter your name and email address.
- You can leave the comment field empty.
- After that, you will be prompted to enter a passphrase to protect your private key.
- After that, GPG will generate a lot of random bytes to generate the key pair.


## Backup key to file
```shell
gpg --armor --export-secret-keys your-email@example.com > my-private-key.asc
```

Use the following command to list the keys you have generated with details:

```shell
gpg --list-keys --keyid-format LONG
```

This will display a list of all the keys you have along with details such as the key IDs, creation dates, and associated emails.
Look for the key you created most recently.

- Identify the Key ID
   From the output, identify the key ID of the latest key. The key ID is usually displayed next to the 'pub' keyword. It will look something like this: rsa4096/1234ABCD1234ABCD 2023-01-01 [SC].

- Export the Specific Key
   Once you have identified the correct key ID, you can export just that key by replacing your-email@example.com with the key ID in the export command. For example:

```bash
gpg --armor --export 1234ABCD1234ABCD > publickey.asc
```

## Convert the keys to base64

```shell
base64 -i publickey.asc -o publickey_base64.asc
base64 -i privatekey.asc -o privatekey_base64.asc
```

## Decrypting a file

```shell
gpg --output decrypted_image.png --decrypt encrypted_image.png
```

## Import a key

Used to import a symmetric key generated by the client-side encryption function in the UI.

```shell
gpg --import decrypted_symmetric_key.asc
```

--- _order.yaml ---
- data-storage
- data-privacy
- data-validation
- data-attestation

--- data-attestation.md ---
---
title: Data Attestation
excerpt: ''
deprecated: false
hidden: false
metadata:
  title: ''
  description: ''
  robots: index
next:
  description: ''
---
Anyone can submit data to the Vana network. However, for data to be considered valid by a DataDAO, it must be attested for by a trusted party. These trusted parties issue an attestation about the data to prove that it is, in fact, authentic, high-quality, unique, and has whatever other properties DataDAOs value in its data contributions.

Data attestations live mostly offchain, and a URL to a data's attestation is written onchain alongside the data itself.

<Image align="center" alt="A DLP that uses TEE validators gets its data contributions attested for (in blue)" border={false} caption="A DataDAO that uses TEE validators gets its data contributions attested for (in blue)" src="https://files.readme.io/76a78110465f22831e3e9358f48af86eb5f0579ebfb1ceb7f16dbf1d32bf70db-image.png" />

### Attestation Schema

The attestation of a data point must follow a spec. Attestations show relevant information about how the data was evaluated, proof-of-contribution scores, integrity checksums, and custom metadata relevant to a specific DataDAO.

An example of when this would be useful: consider a ChatGPT DataDAO that accepts GDPR exports from chatgpt.com. Say the DataDAO considers the export to be high quality when the number of conversations in the export exceeds 10. This DataDAO can insert `numberOfConversations: xxx` in the attestation when Proof of Contribution is run, and anyone can see how valuable that encrypted data point is.

### Schema

* `signed_fields` Contains the main data fields that are signed by the prover.
  * `subject`Information about the datapoint being attested for.
  * `url` URL where the encrypted file lives.
  * `file_id` The ID of the file, given by the Data Registry
  * `owner_address` Wallet address of the file owner.
  * `decrypted_file_checksum` Checksum of the decrypted file for integrity verification.
  * `encrypted_file_checksum` Checksum of the encrypted file for integrity verification.
  * `encryption_seed` The message that was signed by the owner to retrieve the encryption key.
* `prover` Information about the prover.
  * `type` Type of the prover, `satya` is one of the confidential TEE nodes in the Satya network. Proofs can also be `self-signed` where the data owner generates the proof.
  * `address` Wallet address of the prover.
  * `url` URL or address where the prover service is hosted.
* `proof` Details about the generated proof.
  * `image_url` Docker image URL of where the instructions to generate the proof is downloaded from
  * `created_at` Timestamp of when the proof was created.
  * `duration` Duration of the proof generation process, in seconds.
  * `dlp_id` DLP ID from the Root Network Contract, this is used to tie the proof to a DataDAO.
  * `valid` Boolean indicating if the subject is valid.
  * `score*`Overall score of the subject, from 0-1.
  * `authenticity` Authenticity score of the subject, from 0-1.
  * `ownership` Ownership score of the subject, from 0-1.
  * `quality` Quality score of the subject, from 0-1.
  * `uniqueness` Uniqueness score of the subject, from 0-1.
  * `attributes` Additional key/value pairs that will be available on the public proof. These can be used to quickly view properties about the encrypted subject.
  * `metadata*` Key/value metadata about the proof that is written onchain.
* `signature` Generated by the prover signing a stringified representation of `signed_fields`, sorted by the key name. To verify it, we can take the signature and the stringified representation, and extract the address that signed it, which should match the `prover.address`.

>  Note
>
> The `score` and `metadata` fields are written onchain, and a DataDAO can use these fields to calculate how many DataDAO-specific tokens should be issued as a reward to the data contributor for their contribution.

### Sample Attestation

```
{
  "signed_fields": {
    "subject": {
      "file_id": 18,
      "url": "<https://drive.google.com/uc?export=download&id=1E0piiDCaHWgNPqlZdu937pALQt8CeROj">,
      "owner_address": "0x34529235dAF0B317D30F8e3120Ef04Dff59aB411",
      "decrypted_file_checksum": "f5b21ff47184fb726101804ab59deb6b1df28c160b47bec03121489f356ef6a9",
      "encrypted_file_checksum": "33c26529712db36c46aa1bd75d35bfe3b5ec6cfd781ee13694cf327a8861087b",
      "encryption_seed": "Please sign to retrieve your encryption key"
    },
    "prover": {
      "type": "satya",
      "address": "0x2B1A9C62397e77Fa365ef95010e27D4eF3de0c55",
      "url": "<http://172.191.63.53:8091">
    },
    "proof": {
      "image_url": "<https://github.com/vana-com/vana-satya-proof-template/releases/download/v24/gsc-my-proof-24.tar.gz">,
      "created_at": 1727408053,
      "duration": 9.1242094039917,
      "dlp_id": 1234,
      "valid": false,
      "score": 0.8411764705882352,
      "authenticity": 0,
      "ownership": 1,
      "quality": 0.7352941176470588,
      "uniqueness": 0,
      "attributes": {
        "total_score": 0.5,
        "score_threshold": 0.68,
        "email_verified": true
      },
      "metadata": {
        "dlp_id": 1234
      }
    }
  },
  "signature": "0xbd0905b9f41d773b25b7ebeb194fafd4475107e1bc5f520ce1ebfaa0d3c362177cf06f8127a9fe07eec95f02d21f2ed893a473cf88e01e0f701460b848fe324b1c"
}
```

--- data-privacy.md ---
---
title: Data Privacy
excerpt: ''
deprecated: false
hidden: false
metadata:
  title: ''
  description: ''
  robots: index
next:
  description: ''
---
### Encrypting Data

Vana uses a non-custodial encryption technique to encrypt personal data. The network strives to ensure personal data remains private and is only shared with trusted parties. Data does not leave the user's browser unencrypted. A user's file is symmetrically encrypted with their encryption key, and if the encryption key is shared with another party, the key is encrypted with that party's public key so only the intended recipient can decrypt the key and the data.

<Image align="center" border={false} caption="Encrypting user data, and sharing with a trusted party" src="https://files.readme.io/e90e1ae7f0954078d53828cb478c4c0bd31eaacd6073f2606644b87ea0733dc9-image.png" />

The steps are as follows:

1. The user uploads a decrypted file (F).
2. They are prompted to sign a fixed message (the encryption seed) with their wallets, creating a unique signature that can only be recreated by signing that same message using that same wallet.
3. The generated signature is used as the encryption key (EK) to encrypt the file F using a symmetric encryption technique, creating an encrypted file EF.
4. The encryption key EK is then encrypted with the trusted party's public key, making an encrypted encryption key (EEK). This is known as proxy re-encryption. The EEK is stored on the [Data Registry](/docs/data-registry) under the file's permissions and can be retrieved by looking up the trusted party's address.
5. The encrypted file EF and encrypted encryption key EEK can be safely shared with the intended recipient.

<br />

### Decrypting Data

Once the data has been encrypted, it can be decrypted by either a dApp or trusted party.

![](https://files.readme.io/cca6aabb0549c03cb4fb1fad21d5db28823e04b9623812bcbe610df843c5e2c2-image.png)

### Decryption by dApp

1. The dApp prompts the user to sign the same fixed message, to retreive the same EK as above
2. The EK is used to decrypt the encrypted file EF and retreive F

<br />

### Decryption by Trusted Party

1. The trusted party receives the encrypted file EF and the encrypted encryption key EEK
2. They decrypt the EEK using their private key, to retreive the encryption key EK
3. The EK is used to decrypt the encrypted file EF and retreive F

<br />

### Code Samples

Data DAOs can use the encryption technique described above to efficiently encrypt large files (up to several gigabytes in size) in the browser. <Anchor label="Github" target="_blank" href="https://github.com/vana-com/vana-dlp-ui-tee-template/blob/3a803771670fc51af3228109699f7c3210c94d46/app/utils/crypto.ts#L4">Github</Anchor>

If the encryption key EK needs to be shared with a trusted party, it can be encrypted with their public key. To generate a new public/private keypair, see the instructions <Anchor label="here" target="_blank" href="https://github.com/vana-com/vana-dlp-ui-tee-template/blob/main/keys.md">here</Anchor>.

```
import \* as openpgp from "openpgp";

const encryptionKey = "0x0a34ab7..."; // This is a secret (EK)
const publicKeyBase64 = "LS0tL..."; // Trusted party's public key, base64 encoded
const publicKey = await openpgp.readKey({
  armoredKey: atob(publicKeyBase64),
});
const encryptedEncryptionKey = await openpgp.encrypt({
  message: await openpgp.createMessage({ text: encryptionKey }),
  encryptionKeys: publicKey,
  format: "armored",
});
// encryptedEncryptionKey (EEK) is safe to share with the trusted party
```

The trusted party can now receive and decrypt EEK, resulting in EK which can then be used to decrypt the user file F. A Python code example of this is shown <Anchor label="here" target="_blank" href="https://github.com/vana-com/vana-dlp-chatgpt/blob/main/chatgpt/utils/proof_of_contribution.py#L60">here</Anchor>.

--- data-validation.md ---
---
title: Data Validation
excerpt: ''
deprecated: false
hidden: false
metadata:
  title: ''
  description: ''
  robots: index
next:
  description: ''
---
Vana uses a **Proof of Contribution** system to validate data submitted to the network. "Valid" means something different in each DataDAO, because different DataDAOs value data differently.

### Running Proof-of-Contribution in the Satya Network

The recommended way of validating data securely in the Vana Network is by using the Satya Network (also known as **Data Validators Network**), a group of highly confidential nodes that run on special hardware. At a high level, the data contributor adds unverified data, and requests a proof-of-contribution job from the Satya Validators (and pay a small fee to have their data validated). Once validated, the Satya validator will write the proof on chain.

### Proof-of-Contribution Template

To run PoC in the Satya Network, a DataDAO builder must implement a simple proof-of-contribution function using this template.

>  PoC Template
>
> <Anchor label="https://github.com/vana-com/vana-satya-proof-template-py" target="_blank" href="https://github.com/vana-com/vana-satya-proof-template-py">https://github.com/vana-com/vana-satya-proof-template-py</Anchor>

The diagram below explains how this PoC template works.

<Image align="center" alt="Proof-of-contribution running in a Satya node" border={false} caption="Proof-of-contribution running in a Satya node" src="https://files.readme.io/4d7e32cf8d78406f4da74f3f93bd8356b5f7210ddeef939e0f0d2f849913ab69-image.png" />

1. The data contributor adds their encrypted data onchain, via the Data Registry.
2. They request a validation job, paying a small fee. Once a Satya node is available to run the job, they connect directly to the node, and send them the encryption key and the proof-of-contribution docker image that needs to run on the data to validate it.
3. The Satya node receives the key, and downloads the encrypted file, and decrypts it
4. The Satya node places the decrypted file in a temporary, trusted\* location within the TDX environment. The node operator cannot see the contents of this location.
5. The Satya node downloads and initializes a docker container to run the specified proof-of-contribution, and mounts the input and output volumes. The PoC container will have access to the decrypted file.
6. The PoC container runs its validations on the decrypted data, and outputs the attestation. More information on data attestation can be found here: Data Attestation.
7. The Satya node reads the output, and generates the proof.
8. The Satya node writes the proof onchain, and claims the fee as a reward for completing that work.

>  Note
>
> A TDX-protected container runs in Intel's Trust Domain Extensions environment, which provides hardware-level isolation and memory encryption. This represents an evolution from the previous SGX-based approach, offering improved performance and scalability while maintaining strong security guarantees.

### Satya Network Integration

Once a data contributor has uploaded their encrypted file to the Data Registry, it's time to run a proof of contribution job to validate it. The steps below show how to use the Satya Network to validate it.

<Image align="center" alt="Using a Satya node to run proof-of-contribution" border={false} caption="Using a Satya node to run proof-of-contribution" src="https://files.readme.io/1df84e6484bd29d7d765486c055db12ce205d725b4151a8c63cd9f088209eafb-image.png" />

1. Each validation job requires a small fee (which changes based on load). The data contributor can get the current fee by calling `teeFee()` on the `TEE Pool Contract`.
2. The current job fee is returned: ex: `job_fee = 0.01 VANA`.
3. The DataDAO UI now attaches a listener to listen for `JobSubmitted` events from the TEE Pool contract, which emits when the job is successfully submitted.
4. The DataDAO UI submits the job request to the TEE Pool to get the data contributor's file validated: `requestContributionProof(file_id, { value: job_fee })`.
5. The TEE Pool assigns a Satya node to handle the job, and the `JobSubmitted` event is fired.
6. The DataDAO UI receives the JobSubmitted event, and gets the corresponding `job_id`.
7. The DataDAO UI gets the details of the Satya node assigned to the job by calling the TEE Pool's` jobTee(job_id)`.
8. The TEE Pool returns the address of a Satya node, so the UI can connect to it directly: ex: <Anchor label="https://satya-1.com" target="_blank" href="https://satya-1.com">https://satya-1.com</Anchor>.
9. The DataDAO UI sends a `/RunProof` request to the Satya node to begin the validation. It includes the encryption key used by the Satya node to decrypt the file, along with the URL of the proof-of-contribution docker image that will be run to generate the attestation. More information about the request below.
10. The Satya node downloads the encrypted data, decrypts it, and spins up the proof-of-contribution container, which validates the data and generates a result. It then builds the attestation according to the Data Attestation schema, and the proof is uploaded to IPFS.
11. The Satya node sends the proof to the TEE Pool.
12. The TEE Pool contract verifies the proof.
13. The TEE Pool adds the proof to the data registry.
14. The Satya node claims the `job_fee` for completing the job.
15. The TEE Pool releases the `job_fee`.

>  Note
>
> The Satya Network is continuously evolving with the latest in confidential computing technology. The recent transition to Intel TDX provides enhanced performance and scalability while maintaining the security guarantees of hardware-based trusted execution. Do not send sensitive information to the Satya nodes while in testnet.

### Running Proofs on a Satya Node

`POST /RunProof`

Once a Satya node has been selected to run the proof-of-contribution for a data point, the data contributor can talk to that node directly.

**Headers**

| Name         | Value              |
| :----------- | :----------------- |
| Content-Type | `application/json` |

Body

| Name                       | Type   | Required                                            | Description                                                                                                                                                                                                                                                                                                                                                                 |
| :------------------------- | :----- | :-------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `file_id`                  | number | true                                                | File ID from the Data Registry                                                                                                                                                                                                                                                                                                                                              |
| `job_id`                   | number | true                                                | Job ID sent by the JobSubmitted event                                                                                                                                                                                                                                                                                                                                       |
| `encryption_key`           | string | true (if `encrypted_encryption_key` isn't provided) | The symmetric key used to decrypt the file (a wallet signature like `0x1234`...)                                                                                                                                                                                                                                                                                            |
| `encrypted_encryption_key` | string | true (if `encryption_key` isn't provided)           | The symmetric key used to decrypt the file (a wallet signature like `0x1234`...), but encrypted with the Satya node's public key                                                                                                                                                                                                                                            |
| `encryption_seed`          | string | true                                                | The message that was signed to generate the encryption\_key                                                                                                                                                                                                                                                                                                                 |
| `proof_url`                | string | true                                                | The proof-of-contribution docker image URL, ex: `https://github.com/vana-com/vana-satya-proof-template/releases/download/v22/gsc-my-proof-22.tar.gz`                                                                                                                                                                                                                        |
| `env_vars`                 | object | false                                               | Any environment variables that get passed into the proof-of-contribution container as key/value pairs                                                                                                                                                                                                                                                                       |
| `secrets`                  | object | false                                               | Any sensitive environment variables that get passed into the proof-of-contribution container as key/value pairs, encrypted using the process below. These secrets are only decryptable by a registered Satya node.                                                                                                                                                          |
| `nonce`                    | number | false                                               | A random number that will be signed and returned in the response, useful for checking the Satya node's wallet address.                                                                                                                                                                                                                                                      |
| `validate_permissions`     | object | false                                               | When permission is granted to a file to a party, the encryption key is encrypted with the party's public key and then written to the Data Registry. To verify if the permission granted is accurate, send the details of how the encryption key was encrypted. The Satya node will encrypt the encryption key in the same way, and verify that the permission was accurate. |

**Sample Request**

```json json
{
  "job_id": 114815,
  "file_id": 553343,
  "nonce": "1234",
  "proof_url": "https://github.com/vana-com/vana-satya-proof-template/releases/download/v24/gsc-my-proof-24.tar.gz",
  "encryption_seed": "Please sign to retrieve your encryption key",
  "env_vars": {
    "USER_EMAIL": "user123@gmail.com"
  },
  "validate_permissions": [
    {
      "address": "0x0161DFbf70a912668dd1B4365b43c1348e8bD3ab",
      "public_key": "0x075d4a19477220b15b2a955f12dde68ea8811b5500608f76872b88ab494148de49bbac535eb9316a4866225cd073e5f793a93eca718e3749c87d2eb7f9360a85",
      "iv": "8163c393101b852b86ecd9a29f136962",
      "ephemeral_key": "d260671b646f15d4c6a6a954468cb68f269059b933ad80c8194eb52570469ad4"
    }
  ],
  "encrypted_encryption_key": "8163c393101b852b86ecd9a29f1369620442fa291608ec3dddef61e076ace5d4c27ec84abcc96944b4050c3cce5a3962bac7b083c17af840c086fdf3fee011b2a1ba136215da0ab0e72e18fe2a36bc5020f58edd29cb29ea0134a62aad94b0fb74db54d1904a6e0b7260ee6ae224d8682f15b3a54698a9fb358a950a3411d00a12f3ea49774c08e16649512e0bb838ab71cd4c3622be61c9329569fbebe42af4588db0bc51b654efced6f40d54d6c9b9a95bdfe31e168d6492475dd0eb17ebb6aac753c366acd09aae7aa4f667c8e85293ae8c55f8c11445524c1780c5a33582b5b00f2db5dad628461da3e7ee0152000f5e6144ee068e26371c9ea49a93610481"
}
```

**Response**

On success, returns a JSON object containing:

| Name             | Type   | Description                                                            |
| :--------------- | :----- | :--------------------------------------------------------------------- |
| `job_id`         | number | The ID of the job that was processed                                   |
| `file_id`        | number | The ID of the file that was validated                                  |
| `exit_code`      | number | Exit code from the container execution                                 |
| `ipfs_proof_url` | string | Direct URL to access the proof on IPFS gateway                         |
| `proof`          | object | The complete proof object that was uploaded to IPFS                    |
| `logs`           | string | Container execution logs (only available in testnet)                   |
| `signed_nonce`   | string | If a nonce was provided in the request, it will be signed and returned |

**Example Success Response**

```Text 200
{
  "job_id": 123456,
  "file_id": 789012,
  "exit_code": 0,
  "ipfs_proof_url": "https://ipfs.vana.org/ipfs/QmeXfigTXq8D5WfM9UQzNHH5iNxayS1kKPGJPbf3YL4YZi",
  "proof": {
    "signed_fields": {
      "subject": {
        "file_id": 789012,
        "url": "https://example.com/file.json",
        "owner_address": "0x...",
        "decrypted_file_checksum": "...",
        "encrypted_file_checksum": "...",
        "encryption_seed": "..."
      },
      "prover": {
        "type": "satya",
        "address": "0x...",
        "url": "..."
      },
      "proof": {
        "image_url": "...",
        "created_at": 1234567890,
        "duration": 10.5,
        "dlp_id": 123,
        "valid": true,
        "score": 0.85,
        "authenticity": 1.0,
        "ownership": 1.0,
        "quality": 1.0,
        "uniqueness": 1.0,
        "attributes": { ... },
        "metadata": { ... }
      }
    },
    "signature": "0x..."
  },
  "logs": "Container execution logs...",
  "signed_nonce": "0x1234..."
}
```
```Text 400
{
  "error": "Invalid request"
}
```

### Environment Variables in Proof Container

When your proof-of-contribution container runs, it has access to several environment variables:

**System Variables**

<Table align={["left","left","left"]}>
  <thead>
    <tr>
      <th>
        Name
      </th>

      <th>
        Description
      </th>

      <th>
        Example
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        `FILE_ID`
      </td>

      <td>
        The ID of the file being validated
      </td>

      <td>
        1234
      </td>
    </tr>

    <tr>
      <td>
        `FILE_URL`
      </td>

      <td>
        The URL where the encrypted file is stored
      </td>

      <td>
        <Anchor label="https://drive.google.com/uc?export=download&id=1234" target="_blank" href="https://drive.google.com/uc?export=download&id=1234">https://drive.google.com/uc?export=download&id=1234</Anchor>
      </td>
    </tr>

    <tr>
      <td>
        `JOB_ID`
      </td>

      <td>
        The ID of the current validation job
      </td>

      <td>
        1234
      </td>
    </tr>

    <tr>
      <td>
        `OWNER_ADDRESS`
      </td>

      <td>
        The wallet address of the file owner
      </td>

      <td>
        0xabcd1234
      </td>
    </tr>

    <tr>
      <td>
        `VALIDATED_PERMISSIONS`
      </td>

      <td>
        If the `validate_permissions` was provided in the request, this JSON array will contain validated permissions of addresses and public keys of permitted parties.
      </td>

      <td>
        `[{ "address": 0xabcd1234,
                                 "public_key": 0xabcd1234}]`
      </td>
    </tr>
  </tbody>
</Table>

**Custom Variables**

* All key-value pairs provided in the `env_vars` object of the request
* All decrypted secrets from the `secrets` object

**Sending Secrets to PoC Container**

When your Proof-of-contribution container runs, you may need to access secrets such as API keys, passwords, etc. The Satya nodes accept an `env_vars` object to send environment variables in plain text, however, this is not suitable for secret values.

To send secrets, encrypt them with the public key below, and send them as a part of the `secrets` object in the `/RunProof` request. They will be decrypted and injected into the Proof-of-contribution container as environment variables along with the `env_vars`. These secrets can only be decrypted by a TEE that's currently registered in the TEE Pool.

```
# Create the public key

echo "-----BEGIN PUBLIC KEY-----
MIIBojANBgkqhkiG9w0BAQEFAAOCAY8AMIIBigKCAYEA4129oK+dUEalpqP5aT/M
A6yhFbAjNppOidQuVgeSgEPquXLlJrdLoomHGhzugbBYeKS6lceEDM3oygFdCGhT
sly26Ws8qyUIGlk0/JGf4mRHd9RMs0uOF50/mB4abNM/mA/k8cO46+UmXOK2rwEL
U2rPb5tWVzxjPqs8Aw9eT1n7UlvOXxFc4ChyIHX/plfbkKK1R1+PYhtBHeQT8aW1
o7wLsbbnkCGh2iahJaNacMWmUZ9YygdPg2DICQLK2KbZfZHhhylBjDzuBgjUzNai
ikVHzrR6f9eTihYjmpx8Br5Ubhj3lVt45nAXFidxMBe1e7IILNVl9C57sqV+nPFM
2s5ad/r3TDjOZ23e0FGBVsyG+lJwn9q/kx4kjSFsO8fNzJ7wUczVnfW+akox2rMX
rnvdxUhpAAEtJZme5+pnS6Fr4Zi8mUBPt9kC/mHTtbPQoLsX+FeBs/u+rpXe4xBr
+QhqShKWQ+4HzwQHCc5h9d4pqZEKK8UnpdeJ0c/QTqcVAgMBAAE=
-----END PUBLIC KEY-----" > public_key.pem

# Encrypt a sensitive value, ex: "my_password"

echo -n "my_password" | openssl pkeyutl -encrypt -pubin -inkey public_key.pem -pkeyopt rsa_padding_mode:oaep -pkeyopt rsa_oaep_md:sha256 | xxd -p -c 256 | tr -d '\\n'

> 008b24d9c6e4ffc47e3ffa967c09804c7d536d5cf9d4b8a128699917823c9ec4987e82cc71e0bddefc02f273f3b5c48b9ac5238623c3496a865d986542dda00af0f328593243f9369fe1d83cfdadde9da8da319e7b3bb153a5d3d5f41b780def4454e3d4de622d2a60dde568a153d6ad4b7861937a381916786827ada875dc469c2838433d50a6076b1da6af720b696ad452972cc48bec6093738e75e9a49bef3d6f96769c6f1bf14ad2f2115de41133f3043976f2ce257f4cd3f4bafcd597f92bfaa8bf393b038fcdd70fa652bccf45669ac2007689af45c7da91abcb31b326d4d632560cf4857bc1e806c17b33aa6ee6af1f70641f63e487321ddfd6eea9accfac3a72915807a5ff553fc782d823e64f547fff8fb53b9b0022e54e8d78ba6c9973943c2bc491ac0b2020c94f7ae7198451ed295997a8c1e0c7dbe524a0faedd1cadc4149e06d14ecd533262b412def7108cd0dff76e2c00fe7598bb52cdb546afb38962405d5d25a50ff3c377eddf06f056363c41ae870dcb25819b98142e8

# Include the encrypted secret in the POST /RunProof request

...
secrets: {
    password: "008b24d...142e8"
},
...
```

--- index.md ---
---
title: Data Ingress in Detail
excerpt: ''
deprecated: false
hidden: false
metadata:
  title: ''
  description: ''
  robots: index
next:
  description: ''
---
<HTMLBlock>{`
<div style="position: relative; padding-bottom: 56.25%; height: 0;"><iframe src="https://www.loom.com/embed/e4508aae3dfb4af7997204f5f327107d?sid=bc3062d2-5792-43d5-8db3-1e3750af14f3" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe></div>
`}</HTMLBlock>

Anyone can add data to the Vana network. However, for a data point to be considered "valid", it must run through Proof of Contribution to ensure its validity. This process outputs a public attestation that can be attached to an encrypted data point, allowing anyone to judge how valuable a data point is without needing to decrypt it.

![](https://files.readme.io/2c39e2dba416e36664f4dbbbca741a54fd1d29760cfd1bbf0fa087a8990698f5-image.png)

At a high level, these are the steps:

* A **Data Liquidity Pool** provides a UI for its **data contributors** to add data (a webapp, browser extension, CLI tool, etc).
* The data contributor encrypts and uploads the data to a storage location of the DataDAO's choosing.
* The DLP specifies a Proof-of-Contribution function to run on the data. If using the Satya network to run PoC, it will coordinate with a Satya node to generate this attestation.
* The attestation is recorded onchain, and the data contributor is rewarded for their contributions.

Read through this section's subpages to learn more about how each step works.

--- data-storage/_order.yaml ---
- data-storage-options

--- data-storage/data-storage-options.md ---
---
title: Data Storage Options
excerpt: ''
deprecated: false
hidden: true
metadata:
  title: ''
  description: ''
  robots: index
next:
  description: ''
---
Vana empowers you with control and ownership of your data, utilizing on-chain attestations (proofs) while storing the raw data off-chain. To safeguard your information while enabling you to monetize it, Vana adheres to the principles of the CIA Triad set forth by the National Institute of Standards and Technology (NIST) and the National Cybersecurity Centre of Excellence (NCCoE):

**Confidentiality:** Your data is stored in an encrypted format.\
**Integrity:** Fingerprints of your data are securely stored and verified by Trusted Execution Environments (TEEs).
**Availability:** On-chain attestations are perpetually accessible; you must ensure that your encrypted raw data is consistently available online.

While Vanas platform facilitates this process, it's crucial to consider various storage options for your encrypted off-chain data to guarantee its long-term availability.

## Storage Options for Encrypted Raw Data

### 1. Distributed File Storage (IPFS)

<Anchor label="External site link" target="_blank" href="https://ipfs.tech/">External site link</Anchor>

<Anchor label="Integration documentation" target="_blank" href="https://docs.ipfs.tech/">Integration documentation</Anchor>

**Overview:** IPFS (InterPlanetary File System) is a widely used distributed file system that divides files into 256KB blocks. Each block is individually hashed and dispersed across the IPFS network, providing exceptional redundancy and availability. File retrieval is swift, as it can restore blocks from multiple nodes concurrently.

Files stored in IPFS are uniquely identified by their hash which means files are immutable since a file change would create a different file hash. If running a system that writes matching files, such as nodes on a decentralised network performing the same task, there are performance benefits since multiple nodes writing the same file do not create multiple copies since the hash would be the same for each copy.

For Vana, the unique hash means that each dataset is uniquely defined by its hash, with later versions of that dataset having a different hash making it straight forward to uniquely map to dataset versions.

**Cost Considerations:**

* **Self-Hosted Node:** Free to access.
* **Using Providers:** Typically around $0.02 per GB per month plus bandwidth charges.

**Use Cases:** IPFS provides high availability and redundancy without centralized control making it ideal for use with Vana. Ideal for decentralized applications and blockchain projects and for teams looking for decentralized long term storage.

**Distinguishing Features:**

* **Decentralization:** Resistant to censorship.
* **Permanence:** Files remain accessible as long as someone is hosting them.

### 2. Cloud Storage Solutions (Dropbox, Google Drive, OneDrive, Tresorit)

<Anchor label="Dropbox integration documentation" target="_blank" href="https://www.dropbox.com/developers/documentation">Dropbox integration documentation</Anchor>

<Anchor label="Google Drive integration documentation" target="_blank" href="https://developers.google.com/drive/api/guides/about-sdk">Google Drive integration documentation</Anchor>

<Anchor label="OneDrive integration documentation" target="_blank" href="https://learn.microsoft.com/en-us/onedrive/">OneDrive integration documentation</Anchor>

<Anchor label="Tresorit integration documentation" target="_blank" href="https://support.tresorit.com/hc/en-us">Tresorit integration documentation</Anchor>

**Overview:** These popular cloud storage solutions allow you to conveniently provide public access to encrypted training data files. They are user-friendly and often include free tiers.

Cloud storage services such as Dropbox, Google Drive, OneDrive and Tresorit do maintain version histories but they are generally only provided back to user to allow the user to revert to earlier versions of the file. This is often not straight forward and doesn't allow Vana to perform the action of accessing earlier versions of a file via a tag. Therefore users would be required to store different copies of files to provide a similar experience to IPFS, with separate URL's for each version.

DataDAO should also give consideration as to whether data should be held on a cloud share under the DataDAO's cloud subscription or whether users should host their data under their own subscriptions. A DataDAO providing a DataDAO level cloud storage share centralizes the hosting cost and allows the DataDAO to ensure data remains available, while users providing their own hosting decentralizes hosting costs and gives users better control and ownership over their own data.

Services like Tresorit offer high levels of certification and compliance and are targeted at industries that require higher levels of control, compliance and reporting of the data being hosted.

**Cost Considerations:**

* **Dropbox:** Free tier includes 2GB; paid plans start at $9.99/month for 2TB.
* **Google Drive:** Free tier includes 15GB; paid plans start at $1.99/month for 100GB.
* **OneDrive:** Free tier includes 5GB; paid plans start at $1.99/month for 100GB.
* **Tresorit:** Paid plans start at $4.99/month for 50GB.

**Use Cases:** Good option for individuals or small teams looking for easy-to-use solutions which are often natively supported in popular office and data applications, making data exports easy using an applications in-built features or using application plug-ins.

**Distinguishing Features:**

* **Integration:** Seamlessly integrates with other applications (e.g., Google Workspace, Microsoft 365).
* **User-Friendliness:** Very easy to set up and use.

### 3. AWS S3 Buckets

<Anchor label="External site link" target="_blank" href="https://aws.amazon.com/pm/serv-s3">External site link</Anchor>

<Anchor label="Integration documentation" target="_blank" href="https://docs.aws.amazon.com/s3/">Integration documentation</Anchor>

**Overview:** AWS S3 (Simple Storage Service) offers highly scalable object storage with pay-as-you-go pricing. Its excellent for hosting public files and allows customizable permissions.

AWS uses ETAG's to identify file changes but ETAG's are not unique identifiers for file versions. Instead AWS supports a version ID system for identifying unique versions of a file.

`https://<bucket-name>.s3.amazonaws.com/<object-key>?versionId=<version-id>`

**Cost Considerations:**

* **Storage Cost:** Around $0.023 per GB for the first 50TB per month.
* **Request Cost:** Varies based on the number of requests.

**Use Cases:** Good option for enterprises requiring large-scale storage solutions with high durability and availability. Good for teams with the engineering expertise to configure and manage AWS S3 as spart of their wider IT estate.

**Distinguishing Features:**

* **Scalability:** Extremely scalable; handle any size data without performance degradation.
* **Integration Wither Other AWS Services:** Easily store and maintain data from the enterprises wider AWS hosted platform.

### 4. Azure Blob Storage

<Anchor label="External site link" target="_blank" href="https://azure.microsoft.com/en-gb/products/storage/blobs">External site link</Anchor>

<Anchor label="Integration documentation" target="_blank" href="https://learn.microsoft.com/en-us/azure/storage/blobs/">Integration documentation</Anchor>

**Overview:** Similar to AWS S3, Azure Blob Storage provides excellent scalability and public access features. It's cost-effective for large amounts of data.

Azure uses ETAG's to identify file changes but uses version ID's to make it simple to identify different versions of a file.

`https://<account-name>.blob.core.windows.net/<container-name>/<blob-name>?versionid=<version-id>`

**Cost Considerations:**

* **Storage Cost:** Around $0.0184 per GB for hot storage.
* **Transaction Cost:** Additional costs for operations (e.g., read/write requests).
* **Use Cases:** Suitable for applications already using the Azure ecosystem or requiring advanced data analytics. Ideal for creating data packages from data lakes, Microsoft products and services or the enterprises wider estate hosted in Azure.

**Distinguishing Features:**

* **Integration:** Works well with Azures other services (e.g., Azure Functions, Azure AI).
* **Performance Tiers:** Offers different performance tiers based on access frequency.

### 5. Google Cloud Storage

<Anchor label="External site link" target="_blank" href="https://cloud.google.com/storage">External site link</Anchor>

<Anchor label="Integration documentation" target="_blank" href="https://cloud.google.com/storage/docs">Integration documentation</Anchor>

**Overview:** Google Cloud Storage is Googles equivalent to Azure Blob Storage, offering robust public access capabilities and features similar to AWS S3.

Google cloud uses ETAG's to identify file changes but file identities to identify versions of a file.

`https://storage.googleapis.com/<bucket-name>/<object-name>?generation=<generation-number>`

**Cost Considerations:**

* **Storage Cost:** Around $0.026 per GB for standard storage.
* **Access Cost:** Costs for class A and B operations apply.

**Use Cases:** Ideal for businesses heavily invested in the Google Cloud ecosystem. Good for serving large amounts of data and content delivery.

**Distinguishing Features:**

* **Multi-Regional Storage:** Provides high availability and redundancy by storing data across multiple regions.
* **Object Lifecycle Management:** Automatically manages the storage lifecycle of your data, reducing costs.

<br />

## Comparison of Storage Options

| Storage Option            | Pros                                                                                 | Cons                                                                    | Best For                                                                                                                                                                                               |
| :------------------------ | :----------------------------------------------------------------------------------- | :---------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| IPFS                      | - Decentralized, resistant to censorship.                                            | - Technical setup required; availability depends on peers.              | Data providers seeking scalable, decentralised and non-custodial storage, and without an established cloud based ecosystem.                                                                            |
|                           | - Files remain accessible as long as hosted.                                         | - Need to either run a node or pay for a hosting service to run a node. |                                                                                                                                                                                                        |
| Traditional Cloud Storage | - User-friendly, integrates well with services.                                      | - Costs can escalate for large storage; restrictive access management.  | Individuals, small teams needing collaboration and easy setup.                                                                                                                                         |
|                           | - Often has free tiers.                                                              |                                                                         |                                                                                                                                                                                                        |
| AWS S3 Buckets            | - Highly scalable, customizable permissions.                                         | - Costs can accumulate; complex pricing structure.                      | Enterprises requiring large-scale storage or with an established AWS hosted IT estate.                                                                                                                 |
|                           | - Suitable for industries requiring recognised compliance and certification.         |                                                                         | Industries needing ISO/IEC 27001, SOC1, SOC2, SOC3, NIST SP 800-53, FedRAMP, HIPAA, GDPR, PCI DSS, DoD Impact Level 5 and 6 certification as well as Cloud Security Alliance (CSA) STAR certification. |
| Azure Blob Storage        | - Excellent scalability and public access features.                                  | - Requires understanding of Azure for management.                       | Users of the Azure ecosystem or needing to integrate with Microsoft provided services.                                                                                                                 |
|                           | - Suitable for industries with high level compliance and certification requirements. |                                                                         | Industries needing ISO/IEC 27001, SOC1, SOC 2, SOC 3, NIST SP 800-53, FedRAMP, HIPAA, GDPR, PCI DSS, DOD certification, with dedicated compliance and security offerings for government.               |
| Google Cloud Storage      | - Comparable features to AWS S3; robust public access.                               | - Costs can rise with extensive use.                                    | Google Cloud users needing high availability.                                                                                                                                                          |
|                           | - Suitable for industries requiring recognised compliance and certification.         |                                                                         | Industries needing ISO/IEC 27001, SOC1, SOC2, SOC3, NIST SP 800-53, FedRAMP, HIPAA, GDPR, PCI DSS, DOD Impact Level 5 and 6 certification.                                                             |

While the other focuses on IPFS and popular cloud hosting options, the only requirement for hosting data is that it should be always available (resilient) and should have a public URL for accessing hosted data.

This means other hosting options are acceptable including the following.

IBM Cloud Object Storage, DigitalOcean Spaces, Alibaba Cloud Object Storage Service (OSS), Wasabi Hot Cloud Storage, Backblaze B2 Cloud Storage, self hosted file servers and private cloud.

--- data-storage/index.md ---
---
title: Data Storage
excerpt: ''
deprecated: false
hidden: false
metadata:
  title: ''
  description: ''
  robots: index
next:
  description: ''
---
In the Vana network, data is stored encrypted and off-chain in a storage solution of the DataDAO's choice, providing flexibility and control over their data. This approach allows data contributors to utilize familiar platforms such as Dropbox, Google Drive, or decentralized options like IPFS.

The DataDAO can choose to provide a central storage location (ex: a Dropbox account controlled by the DataDAO), or ask the data contributor to store it in their own storage (ex: a Dropbox account controlled by the data contributor).

Vana's system only requires two key pieces of information: a URL pointing to the data's location and an optional identifier that changes when the data is modified (e.g., an <Anchor label="ETAG" target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/ETag">ETAG</Anchor> or last modified date). This ensures data at a particular location has not changed since it was uploaded there.

<Image align="center" alt="Contributors upload files to their personal Dropbox or Google Drive accounts in the [GPT Data DAO](https://www.gptdatadao.org/claim/upload)." border={false} caption="Contributors upload files to their personal Dropbox or Google Drive accounts in the [GPT Data DAO](https://www.gptdatadao.org/claim/upload)." src="https://files.readme.io/128f27e19c26fb5469aaa74b1c06f119dcf9e8057fd61d2f691d88f9c2d52818-image.png" />

<br />

By keeping data off-chain but accessible through these identifiers, Vana maintains a balance between data privacy, user control, and cost efficiency.

### Adding Data

To make data discoverable in the Vana network, it must be written onchain using the Data Registry contract. The data contributor first uploads an encrypted file to a storage provider of their choice, then writes a pointer to that file (the URL) and an optional content integrity hash to the registry.

<Image align="center" alt="Adding data to the data registry" border={false} caption="Adding data to the data registry" src="https://files.readme.io/11f6b2fa1093e88c563f0d2b3a349e9f2b930619bf640a85de4c2d942ec18b61-image.png" />

<br />

**To add data to the Vana Network:**

1. Generate a signature, the `encryption_key`, by asking the data contributor to sign a message, the encryption\_seed
2. Symmetrically encrypt the data using the `encryption_key`. Code samples available in Data Privacy.
3. Upload the encrypted data to a location of your choice. This can be a Web2 storage solution like Google Cloud, Dropbox, etc, or a Web3 solution like IPFS
4. Get the storage URL of the uploaded file
5. Add file to the data registry contract: `addFile(encrypted_data_url)`
6. The data registry returns a `file_id`, which can be used later to look up the file